import asyncio
from langchain_core.messages import BaseMessage
# from pydantic_models.chat_models import ChatReqeust, ChatResponse
from sapie.rag.chathistory.mongo.custom_mongo_chat import initialize_chat_history
from sapie.rag.chathistory.chathistory_utils import ChathistoryUtil
from sapie.rag.retriever.rag_service import RagService
from sapie.models.llm.load_llm import LLMLoader


ragService = RagService()

llm_loader = LLMLoader(config_file="sapie/configs/config.json")
llm_instance = llm_loader.get_llm_instance()
# print(f"llm ëª¨ë¸: {llm_instance.model_path}")


class SapieService:
    def __init__(self):      
        config = {
            # "llm_model_path":"/home/jskim/data_js/test_241226/sapie/models/local_models/Qwen2.5-32B-Instruct-AWQ",
            "llm_model_path":"/home/jskim/data_js/vllm/code/models/Qwen2.5-14B-Instruct-AWQ",
            # "tokenizer_path":"/home/jskim/data_js/test_241226/sapie/models/local_models/Qwen2.5-32B-Instruct-AWQ",
            "tokenizer_path":"/home/jskim/data_js/vllm/code/models/Qwen2.5-14B-Instruct-AWQ",
        }
        chat_history_util = ChathistoryUtil(config)
        self.trimmer = chat_history_util.get_trimmer()


    def get_session_history(self, session_id:str):
        chat_history = initialize_chat_history(session_id=session_id, database_name="saltware", collection_name="chat_histories")

        # # ì˜êµ¬ DB
        # permanat_history = initialize_chat_history(session_id=session_id, database_name="saltware", collection_name="permanent_chat_histories")

        messages = chat_history.messages
        if messages:
            trimmed_messages = self.trimmer.invoke(messages)
            # ê¸°ì¡´ ì„¸ì…˜ ê¸°ë¡ ì‚­ì œ
            chat_history.clear()
            
            ## trimëœ ë©”ì‹œì§€ë“¤ì„ ë‹¤ì‹œ ì €ì¥
            for message in trimmed_messages:
                chat_history.add_message(message)

        return chat_history
    
    
    async def process_chat(self, session_id, query):
        try: 
            print(f"procees_chatì—ì„œ ë°›ì€ session_id: {session_id} / query: {query}")
            # 1. ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë° ê°€ì ¸ì˜¤ê¸°
            chat_history = self.get_session_history(session_id)
            print(f"ë„£ì–´ì¤„ ì±—íˆìŠ¤í† ë¦¬: {chat_history}")

            # 2. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            context = ragService.get_context(query)
            print(f"ì»¨í…ìŠ¤íŠ¸ ì´ ë¬¸ìì—´ ê¸¸ì´: {len(context)}")

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            messages = ragService.generate_chat_prompt_hybrid(query=query, context=context, chat_history=chat_history)
            print(f"í”„ë¡¬í”„íŠ¸: {messages}")
            print(f"í”„ë¡¬í”„íŠ¸ ì´ ë¬¸ìì—´ ê¸¸ì´: {len(str(messages[0]))}")

            # 4. OpenAI API í˜¸ì¶œ
            full_response = ''
            for chunk in llm_instance.call_api(messages=messages):
                # print(f"sapie_service ìª½ ì²­í¬: {chunk}")  # ê° ì²­í¬ í™•ì¸
                chunk_replaced = chunk.replace('\n', 'ğŸ–ï¸')
                full_response += chunk
                yield f"data: {chunk_replaced}\n\n"  # SSE í¬ë§·ì— ë§ê²Œ ë°ì´í„° ì²­í¬ ì „ì†¡ 
                await asyncio.sleep(0.01)  # 0.01ì´ˆ ì§€ì—°
                # asyncio.sleepì´ë‚˜ ë‹¤ë¥¸ ë¹„ë™ê¸° í˜¸ì¶œë¡œ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ "ì–‘ë³´"í•˜ì§€ ì•Šìœ¼ë©´, í˜„ì¬ ì‘ì—…ì´ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ë…ì í•˜ê²Œ ë¨ 
                # ì´ ê²½ìš° ë‹¤ë¥¸ ë¹„ë™ê¸° ì‘ì—…(ì˜ˆ: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì „ì†¡)ì´ ì§€ì—°ë  ìˆ˜ ìˆìŒ
                # ìœ„ ì½”ë“œì—ì„œ yieldë¥¼ ì‚¬ìš©í•˜ë©´ ì²­í¬ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜í•˜ì§€ë§Œ, ì´ë²¤íŠ¸ ë£¨í”„ê°€ ë‹¤ë¥¸ ì‘ì—…(ì˜ˆ: í´ë¼ì´ì–¸íŠ¸ë¡œ ë°ì´í„° ì „ì†¡)ì„ ìˆ˜í–‰í•  ì‹œê°„ì„ í™•ë³´í•˜ì§€ ëª»í•¨
                # asyncio.sleepì€ ë‹¤ìŒ ì‘ì—…ì„ ì‹¤í–‰í•  ê¸°íšŒë¥¼ ì£¼ê¸° ë•Œë¬¸ì— ë°ì´í„°ê°€ í´ë¼ì´ì–¸íŠ¸ë¡œ ì¦‰ì‹œ ì „ì†¡ë  ìˆ˜ ìˆìŒ

            print("==============================")
            print(f"ì§ˆë¬¸:  {query}")
            print("==============================")
            print(f"ë‹µë³€:  {full_response}")

            yield 'data: \u200c\n\n'

            # 6. ì±—íˆìŠ¤í† ë¦¬ ì €ì¥
            chat_history.add_message(BaseMessage(type="human", role="user", content=query))
            chat_history.add_message(BaseMessage(type="ai", role="assistant", content=full_response))

            # return response_text

        except Exception as e:
            raise RuntimeError(f"Error processing chat request: {e}")
